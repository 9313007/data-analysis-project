#!/usr/bin/env python
# coding: utf-8

# In[1]:


import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt


# In[3]:


import pandas as pd

# Importing the dataset
df = pd.read_csv(r'C:\Users\uditm\Downloads\Python_youtube_Analysis-master\Python_youtube_Analysis-master\video_id_info.csv', on_bad_lines='skip')


# In[4]:


df.head()


# In[5]:


## lets find out missing values in your data
df.isnull().sum()


# In[6]:


## drop missing values as we have very few & lets update dataframe as well..
df.dropna(inplace=True)


# In[7]:


df.isnull().sum()


# ### Perform Sentiment Analysis

# In[ ]:


#Sentiment analysis is a way for computers to understand and analyze the emotions expressed in text, like whether it's positive, negative, or neutral.
#eg.
# 1)This video is quite helpful-->Positive sentiment [0,1] more it will close to 1 it will positve sentiment 
# 2)Uable to understand the topic -->Negative sentiment[-1]
# 3) I'm attending the lecture this afternoon.-->Neutral sentiment[0]

#The polarity range refers to the scale used in sentiment analysis to measure the degree of positivity or negativity in text, typically ranging from -1 to 1


# In[8]:


#TextBlob is a Python library for processing textual data. It provides a simple API for common natural language processing (NLP) tasks 
#!pip install textblob
import sys #It's called "sys" because it provides access to system-specific parameters and functions.
get_ipython().system('{sys.executable} -m pip install textblob')


# In[9]:


from textblob import TextBlob


# In[10]:


df.head(6)


# In[11]:


df.shape


# In[12]:


#creating a new DataFrame (sample_df) by selecting the first 1000 rows of an existing DataFrame (df).
#This can be useful for working with a smaller subset of data, such as when you want to perform quick analyses or tests without using the entire dataset.

sample_df = df[0:1000]


# In[13]:


TextBlob("Logan Paul it's yo big day â€¼ï¸â€¼ï¸â€¼ï¸") # normal text box


# In[14]:


TextBlob("Logan Paul it's yo big day â€¼ï¸â€¼ï¸â€¼ï¸").sentiment #attribute


# In[15]:


TextBlob("Logan Paul it's yo big day â€¼ï¸â€¼ï¸â€¼ï¸").sentiment.polarity

### its a neutral sentence !


# In[ ]:


#performing sentiment for each row of comment_text'
#polarity = []--black list

#for comment in df['comment_text']:
    #TextBlob(comment).sentiment.polarity
    #polarity.append(TextBlob(comment).sentiment.polarity)

#if there is black txt then will get the exception error . so avoid the exception we have to use try exception block

#syntax 
#try:
    # Code that might raise an exception
# except :
    # Code to handle the exception


# In[16]:


polarity = []

for comment in df['comment_text']:
    try:
        polarity.append(TextBlob(comment).sentiment.polarity)
    except:
        polarity.append(0)


# In[17]:


len(polarity)


# In[18]:


df['polarity']  = polarity

### Inserting polarity values into comments dataframe while defining feature name as "polarity"


# In[19]:


df.head(5)


# ##  Wordcloud Analysis of your data

# In[ ]:


#Word cloud analysis is a visual representation technique that displays the most frequently occurring words in a text dataset


# In[20]:


filter1 = df['polarity']==1
comments_positive=df[filter1]


# In[21]:


filter2 = df['polarity']==-1
comments_negative= df[filter2]


# In[22]:


#!pip install wordcloud
import sys 
get_ipython().system('{sys.executable} -m pip install wordcloud')


# In[23]:


from wordcloud import WordCloud , STOPWORDS
#Stopwords are common words like "the," "is," and "and" that are often removed from text during analysis because they don't carry significant meaning.


# In[24]:


set(STOPWORDS)
#turns the stopwords list into a unique collection of words for faster processing.


# In[25]:


df['comment_text']


# In[26]:


type(df['comment_text'])


# In[27]:


### for wordcloud , we need to frame our 'comment_text' feature into string ..
# joins all the text data from the 'comment_text' column in the DataFrame 'comments_positive' into a single string, separated by spaces.
total_comments_positive = ' '.join(comments_positive['comment_text'])


# In[28]:


wordcloud = WordCloud(stopwords=set(STOPWORDS)).generate(total_comments_positive)
#This line of code creates a word cloud from the text data in `total_comments_positive`, using a predefined set of stopwords to filter out common words that don't carry significant meaning.


# In[29]:


plt.imshow(wordcloud) 
#The imshow() function in matplotlib is used to display images, and in this case, it's used to display the word cloud generated by the WordCloud library. 
plt.axis('off')


# ### Conclusion-->> positive Users are emphasizing more on best , awesome , perfect , amazing , look , happy  etc..
same for negative 
# In[30]:


total_comments_negative = ' '.join(comments_negative['comment_text'])


# In[31]:


wordcloud = WordCloud(stopwords=set(STOPWORDS)).generate(total_comments_negative)


# In[32]:


plt.imshow(wordcloud) 
plt.axis('off')


# In[ ]:


### Conclusion-->> Negative Users are emphasizing more on Terrible , worst ,horrible ,boring , disgusting etc..


# ##  Perform Emoji's Analysis

# In[33]:


#!pip install emoji==2.10.1
import sys 
get_ipython().system('{sys.executable} -m pip install emoji==2.10.1 ')
## 2.10.0 is a most stable version till date , hence installing this version makes sense !


# In[34]:


import emoji


# In[35]:


emoji.__version__


# In[36]:


df['comment_text'].head(6)


# In[37]:


comment = 'trending ðŸ˜‰'


# In[38]:


[char for char in comment if char in emoji.EMOJI_DATA]
#The code snippet you provided is a list comprehension that filters out characters from a string (comment) if they are present in the emoji.
#EMOJI_DATA dictionary. It's a way to extract emojis from a text string.


# In[39]:


## lets try to write above code in a more simpler & readable way :
emoji_list = []

for char in comment:
    if char in emoji.EMOJI_DATA:
        emoji_list.append(char)

emoji_list


# In[41]:


all_emojis_list = []

for comment in df['comment_text'].dropna(): ## in case u have missing values , call dropna()
    for char in comment:
        if char in emoji.EMOJI_DATA:
            all_emojis_list.append(char)


# In[42]:


all_emojis_list[0:10]# 1st 10 emojis


# In[ ]:


### NOw we have to compute frequencies of each & every emoji in "all_emojis_list"..


# In[43]:


from collections import Counter # collection package


# In[44]:


Counter(all_emojis_list).most_common(10)


# In[45]:


Counter(all_emojis_list).most_common(10)[0]


# In[46]:


Counter(all_emojis_list).most_common(10)[0][0]


# In[47]:


Counter(all_emojis_list).most_common(10)[0][1]


# In[48]:


Counter(all_emojis_list).most_common(10)[1][0]


# In[49]:


Counter(all_emojis_list).most_common(10)[2][0]


# In[50]:


Counter(all_emojis_list).most_common(10)[0][1]


# In[51]:


Counter(all_emojis_list).most_common(10)[1][1]


# In[52]:


Counter(all_emojis_list).most_common(10)[2][1]


# In[53]:


freqs = [Counter(all_emojis_list).most_common(10)[i][1] for i in range(10)]
freqs


# In[54]:


emojis = [Counter(all_emojis_list).most_common(10)[i][0] for i in range(10)]
emojis


# In[55]:


pip install plotly


# In[56]:


import plotly.io as pio
pio.renderers.default = 'iframe_connected'


# In[57]:


import plotly.graph_objs as go
from plotly.offline import iplot  
#use this if your chart is not displaying 
#Plotly is configured to display plots correctly.


# In[58]:


trace = go.Bar(x=emojis , y=freqs)


# In[59]:


iplot([trace])


# In[60]:


## Conclusions : Majority of the customers are happy as most of them are using emojis like: funny , love , heart , outstanding..


# ##  Collect Entire data of Youtube !

# In[61]:


import os


# In[62]:


files= os.listdir(r'C:\Users\swati\Desktop\youtube_Project\YT_additional_data')


# In[63]:


files


# In[64]:


## extracting csv files only from above list ..

files_csv = [file for file in files if '.csv' in file]
files_csv


# In[65]:


#while colllecting the data if you encounter any kind of warning its always good to consider a warning modules.
import warnings
from warnings import filterwarnings
filterwarnings('ignore')


# #### different types of encoding-->>
#     Note : encoding may change depending upon data  , country data , sometimes regional data as well.
#     Fore more inforation on Encoding -- Follow below
# ### https://docs.python.org/3/library/codecs.html#standard-encodingsÂ¶

# In[66]:


#all the csv file i have to store in big data frame

full_df = pd.DataFrame()  
path = r'C:\Users\swati\Desktop\youtube_Project\YT_additional_data'
for file in files_csv:
    current_df = pd.read_csv(path+'/'+file, encoding='iso-8859-1')
    full_df = pd.concat([full_df, current_df], ignore_index=True)


# In[67]:


full_df.shape


# In[68]:


full_df.duplicated() #True will represent the duplicate rows and  False represent the uniques rows.


# In[69]:


full_df[full_df.duplicated()].shape


# In[71]:


full_df = full_df.drop_duplicates() ## lets drop duplicate rows ..


# In[72]:


full_df.shape


# In[73]:


#### a... Storing data into csv ..
full_df[0:1000].to_csv(r'C:\Users\swati\Desktop\youtube_Project\export_data/youtube_sample.csv' , index=False)


# In[74]:


#### b... Storing data into json
full_df[0:1000].to_json(r'C:\Users\swati\Desktop\youtube_Project\youtube_sample.json')


# ##  Which Category has the maximum likes ?

# In[75]:


full_df.head(5)


# In[76]:


full_df['category_id'].unique() #returns an array containing the unique values of the category_id


# In[77]:


## lets read json file ..
json_df = pd.read_json(r'C:\Users\swati\Desktop\youtube_Project\YT_additional_data/US_category_id.json')


# In[78]:


json_df


# In[79]:


json_df['items'][0]

### retrieves the first item (index 0) from the 'items' column of the DataFrame


# In[80]:


#now i want id and title in a dictionary  
cat_dict = {} #empty dict 


for item in json_df['items'].values:     #values here return the array representation 
    ## cat_dict[key] = value (Syntax to insert key:value in dictionary)
    cat_dict[int(item['id'])] = item['snippet']['title'] # snippet here is the sub dict so we have to write this way  ['snippet']['title']


# In[81]:


cat_dict


# In[83]:


full_df['category_name'] = full_df['category_id'].map(cat_dict) 
full_df['category_name']
#maps category IDs in the 'category_id' column of full_df DataFrame to their corresponding category titles using the cat_dict dictionary.


# In[84]:


full_df.head(4) #now you can notice that you have a new feature which is a category name 


# In[ ]:


#which category has the maximum likes ?


# In[85]:


plt.figure(figsize=(12,8))#Creates a new figure with a specified size of 12 inches by 8 inches for better visualization.
sns.boxplot(x='category_name' , y='likes' , data=full_df)
plt.xticks(rotation='vertical')#Rotates the x-axis labels vertically for better readability.


# ## Find out whether audience is engaged or not
# like rate ,dislike , comment_count_rate

# In[86]:


(full_df['likes']/full_df['views'])*100


# In[87]:


full_df['like_rate'] = (full_df['likes']/full_df['views'])*100
full_df['dislike_rate'] = (full_df['dislikes']/full_df['views'])*100
full_df['comment_count_rate'] = (full_df['comment_count']/full_df['views'])*100


# In[88]:


full_df.columns # three things added 'like_rate','dislike_rate', 'comment_count_rate'


# In[89]:


#creating box plot for like rate
plt.figure(figsize=(8,6))
sns.boxplot(x='category_name' , y='like_rate' , data=full_df)
plt.xticks(rotation='vertical')
plt.show()


# ### Analysing relationship between views & likes

# In[91]:


#using Regression plot
#regression plot is nothing but it is the combination of a scatter plot + a regression kine on top of that
sns.regplot(x='views' , y='likes' , data = full_df)

it seems that there is straight line it means views will increase my like will also icrease in a same way.
its is cocept of correlation
# In[92]:


full_df.columns


# In[93]:


full_df[['views', 'likes', 'dislikes']]


# In[96]:


full_df[['views', 'likes', 'dislikes']].corr() ### finding co-relation values between ['views', 'likes', 'dislikes']


# In[97]:


#Now if you want to showcase this correlation table in a vsiualized way , you can use the heatmap
sns.heatmap(full_df[['views', 'likes', 'dislikes']].corr() , annot=True) 
#When annot=True, numerical values are displayed on the heatmap cells


# ##  Which channels have the largest number of trending videos?

# In[98]:


full_df.head(6)


# In[99]:


full_df['channel_title'].value_counts() 
# returns the count of unique values in a Series, providing a frequency distribution of the values.


# In[100]:


### lets obtain above frequency table using groupby approach : 
full_df.groupby(['channel_title']).size()


# In[101]:


cdf = full_df.groupby(['channel_title']).size().sort_values(ascending=False).reset_index() 


#reset_index() is a pandas DataFrame method used to reset the index of a DataFrame. 
#It converts the index labels into a new column and assigns a default numeric index to the DataFrame.


# In[102]:


cdf


# In[103]:


cdf = cdf.rename(columns={0:'total_videos'})


# In[104]:


cdf


# In[ ]:





# In[ ]:





# In[ ]:




